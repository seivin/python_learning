# Common activation functions

  tf.nn.sigmoid(z)
  tf.nn.tanh(z)
  # Rectifised Linear Unit = ReLU 
  tf.nn.relu(z)

